---
title: "Testing Turelli and Barton (1990) theory"
author: "Evan Koch"
date: "April 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The theory of Turelli and Barton (1990) provides expressions for the expected changes in trait values under different forms of selection. Here I will attempt to validate that this theory works at the most basic level. Turelli and Barton calculate that the change in the mean phenotype due to one generation of selection is given by 
$$
\Delta \bar{Z} = V_gL_1 + M_{3,g}L_2 + \gamma_4V^2_gL_3 +
  \left( M_{5,g}-4M_{3,g}V_g\right)L_4 + \ldots.
$$
In this expression, $V_g$ is the additive genetic variance, $M_{i,g}$ is the ith central moment of the distribution of breeding values, and $\gamma_4$ is the excess kurtosis of the distribution of breeding values. I am going to simulate example populations, subject them to selection and ensure that the change in the mean phenotype is given given by this equation. Let's start by simulating mutations from a Guassian distribution and assigning them randomly per locus to individual haplotypes (so there's no LD). 
```{r}
library(ggplot2)
make.pop <- function(NN=2000, ll=10, mut.dist=rnorm, ...){
    #  Generate a random effect for each locus from the mutational distribution
    #& using the parameters passed to make.pop
    mut.effect <- mut.dist(n=ll, ...)
    #  Generate allele frequencies for all loci from a beta distribution
    freqs <- rbeta(ll, shape1=0.5, shape2=0.5)
    #  Now assign alleles at random to haplotypes
    alleleic.states <- replicate(n=NN, expr=rbinom(n=ll, size=1, prob=freqs))
    #  Each row of this will correspond to an allele and now we 
    #& just have to multiply each row by the alleleic affect
    alleleic.effects <- t(alleleic.states * mut.effect)
    individual.effects <- t(alleleic.states) %*% mut.effect
    return(list(ae=alleleic.effects, ie=individual.effects, me=mut.effect, freqs=freqs))
}
# Now test this and plot the distribution of breeding values
test.pop.1 <- make.pop(mean=0, sd=1)
test.pop.2 <- make.pop(NN=2000, ll=100, mean=0, sd=1)
test.bvals <- data.frame(cbind(test.pop.1[[2]], test.pop.2[[2]]))
colnames(test.bvals) <- c("loci.10", "loci.100")
test.hist <- ggplot(test.bvals) + xlab("phenotype value") +
    geom_histogram(aes(x=loci.10, fill="10"), 
                   alpha=0.6, binwidth=.25) + 
    geom_histogram(aes(x=loci.100, fill="100"), 
                   alpha=0.6, binwidth=.25) + theme_bw() +
    scale_fill_manual("num loci", values=c("red", "blue"))
test.hist
```

One thing that we can see just from this small test is that, even though the mutational distribution is centered at zero, the mean breeding values are nonzero in either the population with 10 loci affecting the trait or in the population with 100 loci affecting the trait. Indeed, we do not expect the variance in the mean phenotypic value to go to zero over evolutionary realizations as the number of loci gets large. This is a somewhat counterintuitive result since the law of large numbers would seem to suggest that this variance should go to zero! One way to rationalize this is by realizing that there will necessarily be covariance in breeding values due to shared ancestry. It should be helpful to think of a more population genetics interpretation of this, but I'll save that for later. Instead I'll do a quick derivation just to show that this is the case and ensure the variance in the mean phenotype matches theory.

The mean phenotype in the population is
$$
\bar{Y} = \frac{1}{N}\sum_{i=1}^{N}Y_i
$$
where 
$$
Y_i = \sum_{l=1}^L x_{i,l}m_l.
$$
Here, $x_{il}$ is an indicator variable that is one if the individual $i$ carries the phenotype-affecting allele at site $l$ and zero otherwise. $m_l$ is the effect of the allele. For simplicity I'll only consider mutational distributions with mean zero, but the result should generalize. The population variance can then be written as 
$$
Var[\bar{Y}] = \frac{1}{N^2}\left( N Var[Y_i]  + N(N-1) Cov[Y_i,Y_j]\right).
$$
The first term will go to zero as the population size gets large and so is not of much interest. The second will be 
$$
Cov[Y_i,Y_j] = L Cov[m_l x_{i,l}, m_l x_{j,l}] + 
L(L-1) Cov[m_{l_1} x_{i,l_{1}}, m_{l_1} x_{i,l_{2}}],
$$
where
$$
Cov[m_l x_{i,l}, m_l x_{j,l}] = Var[m_l]E[x_{i,l}]^2 = Var[m_l](Var[x] + E[x]^2),
$$
and
$$
Cov[m_{l_1} x_{i,l_{1}}, m_{l_2} x_{i,l_{2}}] = 0
$$
because there is no correlation in the mutational affects at different sites. $Var[x]$ refers to the variance in the population allele frequency distribution. We can thus say that 
$$
Var[\bar{Y}] \approx L Var[m_l](Var[x] + E[x]^2)
$$
Let's test this prediction in the current set of trait simulations. I simulated with mutation effects drawn from a standard normal distribution, so $Var[m_l]=1$ and with allele frequencies drawn $x \sim \mbox{Beta}(\alpha=0.5,\beta=0.5)$. This means that $Var[x]=0.125$ and $E[x]^2=0.25$. Now let's simulate a lot of populations and see what the variance of the mean breeding values is.
```{r}
reps <- 1000
ll.set <- c(10,20,50,100,200,500,1000,1500,2000)
e.var <- ll.set*1*(0.125+0.25)
sim.var <- lapply(ll.set, function(ll){
    means <- replicate(reps, mean(make.pop(NN=100, ll=ll, mean=0, sd=1)$ie))
    return(means)
})
vars <- unlist(lapply(sim.var, var))
ggplot(data=data.frame(cbind(e.var, vars)), aes(x=e.var, y=vars)) + geom_point() + 
    scale_x_log10() + scale_y_log10() + geom_abline(intercept = 0, slope = 1) + 
    theme_bw()
```

It therefore looks like this is working properly and the increase in variance of the mean breeding value across evolutionary replicates is as we would expect. 

Satisfied that we are simulating breeding values in a appropriate way, we now want to simulate populations and apply selection to them. We will start by applying a simple model of exponential selection. The fitness function for exponential selection is 
$$
w(z) = e^{sz}.
$$
This will assign a fitness to each individual and we will sample the next generation proportionally to the fitness values of the parents. This is using either a model of no recombination, or this may represent the population after selection but before recombination.
```{r}
s.test <- 0.1
exp.sel <- function(x, s=s.test) return(exp(s*x))
select.pop <- function(ind.eff, fit.func=exp.sel, ancestral.val=0, ...){
    fitness.values <- apply(ind.eff, MARGIN=1, FUN=function(x.set){
        return(fit.func(sum(x.set) + ancestral.val, ...))})
    #  Now sample rows for a new matrix proportionally to these fitness values
    NN <- nrow(ind.eff)
    sel.indicies <- sample.int(n=NN, size=NN, replace=TRUE,
                           prob=fitness.values/sum(fitness.values))
    after.sel <- ind.eff[sel.indicies,]
    return(after.sel)
}
#  Plot this to see how selection has worked!
test.select <- select.pop(test.pop.2[[1]])
ts <- data.frame(cbind(before.sel=as.numeric(test.pop.2$ie),
                       after.sel=rowSums(test.select)))
ggplot(ts) + geom_histogram(aes(x=before.sel, fill="before sel"), alpha=0.5) +
    geom_histogram(aes(x=after.sel, fill="after sel"), alpha=0.5) +
    scale_fill_manual("pop state", values=c("red", "blue")) + xlab("phenotype value") +
    theme_bw()
remove(test.select)
```

As we would hope, the mean breeding value in the population increases after selection. We now want to perform this selection a large number of times and see how the change in mean phenotype corresponds to that expected from the Turelli and Barton theory. Under exponential selection they calculate that $L_1=s$ and $L_2\approx s^2/2$ while all other $L_k$ are close to zero. 
```{r}
library(moments)
e.dz <- s.test * var(test.pop.2$ie) + s.test^2 * moment(test.pop.2$ie, 
                                                   order=3, central=TRUE)
test.rep.1 <- replicate(500, select.pop(test.pop.2$ae))
test.rep.1.p <- apply(test.rep.1, MARGIN=3, rowSums)
test.rep.1.means <- apply(test.rep.1.p, MARGIN=2, mean)
ggplot(data.frame(dz=test.rep.1.means - mean(test.pop.2$ie))) + 
    geom_histogram(aes(x=dz), alpha=0.7) + theme_bw() + 
    geom_vline(xintercept=e.dz, color="red") + 
    geom_vline(xintercept=mean(test.rep.1.means - mean(test.pop.2$ie)))
remove(test.rep.1)
```

As we can see, the simulated change in phenotype due to selection agrees with the theory, indicating that we are doing everything well so far. Let's do a more extensive test of this though to ensure that the agreement at these parameter values isn't a fluke.
```{r}
library(plyr)
#  Simulate a large number of populations
n.evol <- 1000
test.pops <- replicate(n.evol, make.pop(NN=1000, ll=20, mean=0, sd=1)$ae)
#  Apply exponential selection to these populations
sel.pops <- alply(test.pops, 3, select.pop)
before.var <- unlist(alply(test.pops, 3, function(X) var(rowSums(X))))
before.m3 <- unlist(alply(test.pops, 3, function(X) moment(rowSums(X), 
                                                    order=3, central=TRUE)))
before.m <- unlist(alply(test.pops, 3, function(X) mean(rowSums(X))))
after.m <- unlist(lapply(sel.pops, function(X) mean(rowSums(X))))
e.dzs <- s.test * before.var + s.test^2 * before.m3
o.dzs <- after.m - before.m
exp.test <- data.frame(cbind(exp.resp=e.dzs, obs.resp=o.dzs,
                             var.before=before.var))
ggplot(exp.test) + 
    geom_point(aes(x=exp.resp, y=obs.resp, size=var.before), alpha=0.1) + 
    theme_bw() + geom_abline(intercept=0, slope=1)
remove(test.pops)
```

# Cubic selection
As a test of the potential importance of deviations from normality on the response to selection I'll consider a cubic selection function. We'll investigate what parameter space we get a good description of the response to selection.
```{r}
# Use the same simulated population as before
pop <- test.pop.2
# Define a cubic selection function
cub.sel <- function(x, b0, b3, xm=0) return(ifelse(b0 + b3*(x-xm)^3 < 0, 0, b0 + b3*(x-xm)^3))
```
We are going to want to use the parameter $b_0$ to scale things so that fitness values don't go negative. This will give us a baseline fitness value and $b_3$ determines how steep the cubic selection is.
```{r}
test.vals <- sort(pop$ie)
test.mean <- mean(test.vals)
plot(test.vals, cub.sel(test.vals,50,.01,xm=test.mean), type="l", ylim=c(-0,150), 
     xlab="breeding value", ylab="fitness")
points(test.vals, cub.sel(test.vals,50,.05,xm=test.mean), type="l", lty=2)
points(test.vals, cub.sel(test.vals,50,.1,xm=test.mean), type="l", lty=3)
legend("topleft", legend=c("b3=0.01", "b3=0.05", "b3=0.1"), lty=c(1,2,3))

pdf("cub_shape.pdf")
plot(test.vals, cub.sel(test.vals,50,.01,xm=test.mean), type="l", ylim=c(-0,150), 
     xlab="breeding value", ylab="fitness", lwd=3)
points(test.vals, cub.sel(test.vals,50,.05,xm=test.mean), type="l", lty=2, lwd=3)
points(test.vals, cub.sel(test.vals,50,.1,xm=test.mean), type="l", lty=3, lwd=3)
legend("topleft", legend=c("b3=0.01", "b3=0.05", "b3=0.1"), lty=c(1,2,3), lwd=rep(3,3))
dev.off()
```

Let's also check how fitness values are distributed in this test population.
```{r}
hist(cub.sel(test.vals, 50, .01, test.mean)/mean(cub.sel(test.vals, 50, .01,test.mean)))
hist(cub.sel(test.vals, 50, .05, test.mean)/mean(cub.sel(test.vals, 50, .05, test.mean)))
hist(cub.sel(test.vals, 50, .1, test.mean)/mean(cub.sel(test.vals, 50, .1, test.mean)))
```

This distribution of fitness values is not too crazy, so these look like they might be reasonable test parameters. Of course this function has sort of a strange shape, but we're going to forge ahead and hope that it's not too unrealistic. We could hope it would be an ideal situation for higher order moments of the distribution to matter because the fitness function is very flat in the range of the ancestral mean and begins to change rapidly as you move into the tails. Now we're going to select on the test population as before to see how things look.
```{r}
cube.seltest <- select.pop(pop$ae, fit.func=cub.sel,b0=50, b3=0.05)
cube.bf.af <- data.frame(before=pop$ie, after=apply(cube.seltest, 1, sum))
ggplot(cube.bf.af) + geom_histogram(aes(x=before, fill="before sel"), alpha=0.5) + 
    geom_histogram(aes(x=after, fill="after sel"), alpha=0.5) + 
    scale_fill_manual("pop state", values=c("red", "blue")) + xlab("phenotype value") +
    theme_bw() + geom_vline(xintercept=mean(apply(cube.seltest, 1, sum)), color="red") + 
    geom_vline(xintercept=mean(pop$ie), color="blue")
```

We can see that the change in the mean is rather small, but also that there is a shift in the phenotype distribution that is distinc from the two previous modes of selection we considered. Let's now write down equations that should tell us what the response to selection should be. We assume that the fitness function on breeding values is

$$
W_g(Y) = b_0 + b_3(Y-\bar{Y})^3.
$$
In this fitness function, the cubic curve is shifted so that the inflection point is at the mean breeding value in the population and the baseline fitness at htis value is $b_0$. This gives a scenario where, even though we are assuming that the evolution of the breeding values is neutral, the selection is centered on the \textit{random} populatio mean. $b_3$ determines how steep the increase in fitness is away from the population mean in one direction and how steep the decrease is in the other. We can calculate the approximate mean fitness and the selection gradients in the same way as before. In taking derivatives however we assume that $\bar{Y}$ does not depend on $Y$. 
$$
\bar{w} = b_0 + M_{3,g}b_3,
$$
$$
L_1 = \frac{3V_gb_3}{\bar{w}},
$$
$$
L_2 = 0,
$$
$$
L_3 = \frac{b_3}{\bar{w}}.
$$
Combining these gradients into a single term for the response to selection we get.
$$
\Delta Y = \frac{3 V_g^2b_3 + (M_{4,g}-3V_g^2)b_3}{b_0 + M_{3,g}b_3} = 
\frac{M_{4,g}b_3/b_0}{1 + M_{3,g}b_3/b_0} = \frac{M_{4,g}\beta}{1 + M_{3,g}\beta}
$$
We can see that the response to cubic selection depends only on $\beta$, the ratio of the steepness of cubic selction to the baseline fitness, as well as the third and fourth moments of the distribution of breeding values. It's important to note that we're considering selection on the breeding values directly here. This equation says more generally that the response to cubic selection depends linearly on the fourth moment of the phenotype distribution.
```{r}
wbar <- function(b0, b3, zbar, Vg, M3g){
    return(b0 + M3g*b3)
}
L1.3 <- function(b0, b3, zbar, Vg, M3g){
  return(
    (Vg * 3*b3) / wbar(b0, b3, zbar, Vg, M3g)
  )
}
L2.3 <- function(b0, b3, zbar, Vg, M3g){
  return(
    0
  )
}
L3.3 <- function(b0, b3, zbar, Vg, M3g){
  return(
    b3 / wbar(b0, b3, zbar, Vg, M3g)
  )
}
e.dz.3 <- function(b0, b3, zbar, Vg, M3g, M4g){
  l1 <- L1.3(b0, b3, zbar, Vg, M3g)
  l2 <- L2.3(b0, b3, zbar, Vg, M3g)
  l3 <- L3.3(b0, b3, zbar, Vg, M3g)
  return(Vg * l1 + M3g*l2 + (M4g - 3 * Vg^2) * l3)
}
```

Now as before let's repeat the selection on this one test population to get a sense for how well our theory agrees with the result.
```{r}
e.change <- e.dz.3(b0=50, b3=0.05, zbar=mean(pop$ie), Vg=var(pop$ie), 
                   M3g=moment(pop$ie, order=3, central=TRUE),
                   M4g=moment(pop$ie, order=4, central=TRUE))
cube.tests <- replicate(500, select.pop(pop$ae, 
                                        fit.func=cub.sel, b0=50,
                                        b3=.05, xm=mean(pop$ie)))
cube.test.vals <- apply(cube.tests, 3, rowSums)
cube.test.means <- apply(cube.test.vals, 2, mean)
ggplot(data.frame(dz = cube.test.means - mean(pop$ie))) + geom_histogram(aes(x=dz)) + theme_bw() + 
  geom_vline(xintercept=e.change, color="red") + geom_vline(xintercept = mean(cube.test.means - mean(pop$ie)))
```

Thankfully in this first test our equation seems to do very well. Now we'd like to know the range of $b_3$ for which the approximation works well. 
```{r}
b3.set <- c(0.001,0.01,0.02,0.05,0.1,0.2,0.5,1.0)
sel.outcome <- lapply(b3.set, function(b3){
    simulated <- c(); predicted <- c()
    for(i in 1:50){
        pop.tmp <- make.pop(NN=2000, ll=100, mean=0, sd=1)
        mean.tmp <- mean(pop.tmp$ie)
        var.tmp <- var(pop.tmp$ie)
        M3.tmp <- moment(pop.tmp$ie, order = 3, central=TRUE)
        M4.tmp <- moment(pop.tmp$ie, order = 4, central=TRUE)
        sel.tmp <- select.pop(pop.tmp$ae, fit.func=cub.sel, b0=50, b3=b3, xm=mean.tmp)
        remove(pop.tmp)
        m.post.sel <- mean(rowSums(sel.tmp))
        simulated[i] <- m.post.sel - mean.tmp
        predicted[i] <- e.dz.3(b0=50, b3=b3, zbar=mean.tmp, Vg=var.tmp, 
                   M3g=M3.tmp,M4g=M4.tmp)
    }
    return(data.frame(sim=simulated, pred=predicted))
})
full <- do.call("rbind", sel.outcome)
full$b3 <- unlist(lapply(b3.set/50, function(x) rep(x, 50)))

#pdf("cubic_sel.pdf", height=5, width=10)
ggplot(full) + geom_point(aes(x=pred, y=sim)) + facet_grid( ~ b3, scales="free") +
        geom_abline(slope=1,intercept = 0) + theme_bw()
#dev.off()
```