We can use the low mutation rate approximation to the moment generating function
(equation \ref{eq:mgf_approx_1}) to calculate moments of the distribution of
trait vales. We'll start by calculating the first and second moments and compare
these to the mean and variance of the normal distribution. We start, as we did
in deriving the normal distribution, by substituting the Taylor series of the
mutational mgf.
\begin{equation}
  \label{eq:mgf_approx_sub}
  \varphi_{\mathbf{Y}}(\mathbf{k}) \approx \left[ 1 + \sum_{\omega \in \mathcal{O}}
    E[t_\omega] \T \left( m_1 \sum_{a \in \omega} k_a +
    \frac{m_2}{2!}\left( \sum_{a \in \omega} k_a\right)^2 +
    \frac{m_3}{3!}\left( \sum_{a \in \omega} k_a\right)^3 +
    \frac{m_4}{4!}\left( \sum_{a \in \omega} k_a\right)^4 \ldots \right) \right]^L
\end{equation}
We can expand this out using multinomial coefficients to get
\begin{align}
  \label{eq:mgf_approx_expand}
  \varphi_{\mathbf{Y}}(\mathbf{k}) &\approx 1 +
  L\T \sum_{\omega \in \mathcal{O}} E[t_{\omega}]\left( m_1 \sum_{a \in \omega} k_a +
  \frac{m_2}{2}\left( \sum_{a \in \omega} k_a\right)^2 + \ldots \right) \nonumber \\
  &+ \frac{L(L-1)}{2} \left(\T\right)^2 \sum_{\omega \in \mathcal{O}} E[t_{\omega}]^2
  \left( m_1 \sum_{a \in \omega} k_a +
  \frac{m_2}{2}\left( \sum_{a \in \omega} k_a\right)^2 + \ldots \right)^2 \nonumber \\
  &+ L(L-1)\left(\T\right)^2\sum_{\omega_1, \omega_2 \in \mathcal{O}}E[t_{\omega_1}]E[t_{\omega_2}]
  \left( m_1 \sum_{a \in \omega_1} k_a + \ldots \right)
  \left( m_1 \sum_{a \in \omega_2} k_a + \ldots \right) + \ldots.
\end{align}
The first coefficient is $\binom{L}{L-1,1,\mathbf{0}}$, the second is
$\binom{L}{L-2,2,\mathbf{0}}$, and the third is $\binom{L}{L-2,1,1,\mathbf{0}}$.
This is probably not the best way to think about the combinatorics, but it will
do for now. To calculate the moments of this distribution one takes the partial
derivatives of the mgf and sets the dummy variables to zero.
\begin{equation}
  \label{eq:deriv}
  E[Y_1^{r_1}\ldots Y_n^{r_n}] = \frac{\partial^{r_1 + \ldots + r_n}}{\partial k_1^{r_1} \ldots \partial k_n^{r_n}}
  \varphi_{\mathbf{Y}}(\mathbf{k})\Bigr|_{\mathbf{k}=0}
\end{equation}
Using this to calculate the first moment of the trait distribution we get
\begin{equation}
  \label{eq:mom1}
  E[Y_a] \approx L\T m_1 \sum_{\omega : a \in \omega} E[t_\omega].
\end{equation}
The second moment is more complicated because there are $k_a^2$ terms in all
three lines of equation \ref{eq:mgf_approx_expand}.
\begin{align}
  E[Y_a^2] &\approx L\T m_2 \sum_{\omega : a \in \omega} E[t_\omega] \nonumber \\
  &+ \frac{L(L-1)}{2} \left(\T \right)^2 m_1^2 \sum_{\omega : a \in \omega} 2 E[t_\omega]^2 \nonumber \\
  &+ L(L-1) \left(\T \right)^2 m_1^2 \sum_{\omega_1 , \omega_2: a \in \omega_1 , \omega_2} 2 E[t_{\omega_1}]E[t_{\omega_2}]
\end{align}
Terms with $(\T)^2$ are kept because they also include a second order term of
$L$ in front of them. We can now calculated the variance using $Var[Y]=E[Y^2] -
E[Y]^2$. The squared first moment can be written as
\begin{align}
  \left(L\T m_1 \sum_{\omega : a \in \omega} E[t_\omega] \right)^2 &=
  L^2\left(\T\right)^2 m_1^2 \sum_{\omega : a \in \omega} E[t_\omega]^2 \nonumber \\
  &+ L^2\left(\T\right)^2 m_1^2 \sum_{\omega_1 , \omega_2: a \in \omega_1 , \omega_2} E[t_{\omega_1}]E[t_{\omega_2}].
\end{align}
Subtracting this from the second moment gives
\begin{align}
  Var[Y_a] &\approx L\T m_2 \sum_{\omega : a \in \omega} E[t_\omega] \nonumber \\
  &- L \left(\T\right)^2 m_1^2 \sum_{\omega : a \in \omega}E[t_\omega]^2 \nonumber \\
  &-  2L \left(\T\right)^2 m_1^2 \sum_{\omega_1 , \omega_2: a \in \omega_1 , \omega_2} E[t_{\omega_1}]E[t_{\omega_2}] \nonumber \\
  &\approx L\T m_2 \sum_{\omega : a \in \omega} E[t_\omega] \nonumber \\
  &= L\T m_2 E[t_{MRCA}].
\end{align}
The $(\T)^2$ terms are no only first order in $L$ so they can be ignored. This
is the same variance as in the normal distribution because the limit takes care
of these terms automatically.
%%% Local Variables:
%%% TeX-master: "notes.tex"
%%% End:
