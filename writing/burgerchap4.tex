\begin{flushright}
\textbf{1. Continuum-of-alleles model} \\
\end{flushright}

After introducing Kimura's integro-differential equation for the evolution of the distribution of a quantitive trait in
population (equation IV.1.3), B{\"u}rger states that a simple property of the equilibrium solution is that
\begin{equation}
-\mu \leq \hat{\overline{m}} < 0.
\end{equation}
This follows first from the assumption that the mutational distribution and the fitness function are symmetric about
zero, so the mean \emph{phenotype} in the population will be zero.  Since the fitness function, $m(x)$ is always
negative, $\hat{\overline{m}} < 0$. The $-\mu \leq \hat{\overline{m}}$ half follows from
\begin{equation}
p(x)\left[ \mu + \hat{\overline{m}} - m(x) \right] = \mu \int u(x-y) p(y) dy \geq 0,
\end{equation}
\begin{equation}
\mu + \hat{\overline{m}} = \mu \int u(x-y) p(y) dy / p(x) + m(x).
\end{equation}
Because $m(x) \leq 0$, 
\begin{equation}
\mu + \hat{\overline{m}} \geq \mu \int u(x-y) p(y) dy / p(x) \geq 0.
\end{equation}
Which means $-\mu \leq \hat{\overline{m}}$. The biological implication of this is that the mean decrease in fitness due to
mutation in a population under stabilizing selection is less than the mutation rate. Since we think of per-locus
mutation rates as being quite small, this seems to imply a low per-locus genetic load. This is reminiscent of classic
results saying that the genetic load depends only on the mutation rate up to an approximation.

Next, B{\"u}rger introduces two approximations to the solution of equation IV.1.3:
\begin{equation} \label{eq:1.3}
  \frac{\partial p(x,t)}{\partial t} = \left[ m(x) -
    \overline{m}(t) \right]p(x,t) + \mu \left[ \int u(x-y)p(y,t)dy -p(x,t)\right].
\end{equation}
The first approximation is from \citet{Kimura1965} and called the \emph{Gaussian Allelic Approximation}. Starting with
the equilibrium condition
\begin{equation} \label{eq:equi}
\left[ \mu + \overline{m} - m(x) \right] = \mu \int u(x-y) p(y)dy, 
\end{equation}
Kimura performs the transformation $y \to y + x$ to get
\begin{equation} \label{eq:trans}
\int u(x-y) p(y) dy = \int u(-y)p(y+x)dy.
\end{equation}
Taking the Taylor series around $x$, 
\begin{equation}
p(y+x) \approx p(x) + \frac{dp(x)}{dx}\left( x-(y+x) \right) + \frac{1}{2} \frac{d^2p(x)}{dx^2}\left( x-(y+x) \right)^2
\end{equation}
we get
\begin{equation}
\int u(-y)p(y+x)dy \approx p(x) + \frac{dp(x)}{dx} \int u(-y)y dy + \frac{1}{2} \frac{d^2p(x)}{dx^2}\int u(-y)y^2dy.
\end{equation}
when plugging this into equation \ref{eq:trans}. This approximation will be good if the higher order terms of the
mutational distribution, $u$ are small. If we assume that $u$ has mean zero and variance $\gamma^2$ the plugging in to 
equation \ref{eq:equi} gives
\begin{equation}
s\left( x^2 - \int y^2p(y)dy\right)p(x) = \frac{1}{2}\mu\gamma^2\frac{d^2p(x)}{dx^2}
\end{equation}
after some rearranging. Kimura looked up the solution to such an equation, which happens to be Gaussian with mean zero
and variance $\sqrt{\frac{\mu}{s}\frac{\gamma^2}{2}}$. This is interesting because phenotypic distribution depends on
the mutation rate, mutational variance, and strength of selection.

The second equilibrium approximation is from \citet{Turelli1984}, which makes the approximation 
\begin{equation}
\int u(x-y)\hat{p}(y)dy \approx u(x), 
\end{equation}
which corresponds to the variance of the mutational distribution being much greater than that of the equilibrium
phenotypic distribution. If this is true then 
\begin{equation}
\hat{p}(x) = \frac{\mu m(x)}{\mu + \hat{\overline{m}} - m(x)}.
\end{equation}
All that remains is to find $\hat{\overline{m}}$ such that this integrates to one. Using the quadratic fitness function
$m(x)=-sx^2$ and $\mu/(s\gamma^2) \to 0$, \citet{burger2000mathematical} finds the variance of this distribution to be
\begin{equation}
\sigma^2 = \frac{\mu}{s} - \frac{\pi}{2\gamma^2}\left( \frac{\mu}{s^2}\right), 
\end{equation}
Although the further approximation $\sigma^2=\mu/s$ is what Turelli originally found.

\begin{flushright}
\textbf{Thoughts on this stuff} \\
\end{flushright}

All of this is very interesting, but the obvious question comes up about how these models behave when extended to
multiple loci as is more realistic for most traits. Towards that end it will be interesting to read \citet{Lande2007}.
An additional question is how to intuitively understand the differences between these models and what that means in terms 
of the empirical study of quantitative characters. 

\begin{flushright}
\textbf{2. The general model and its basic properties}\\
\end{flushright}

Burger notes that equation \ref{eq:1.3} is similar to the mutation-selection equations for a finite number of alleles
\begin{equation}
\dot{p_i}=p_i(m_i-\overline{m}) + \sum_j\left( p_jb_j\mu_{ji}-p_ib_i\mu_{ij}\right),
\end{equation}
and that for a stepwise mutatinoal model. The main point here is that we can likely generalize the notion that the
change in frequence of a type is determined by its fitness relative to the population mean and some sort of
transformation due to selection. We can do this for any sort of discrete or continuous gene effects.

We give each type $x$ a mutation rate $\mu(x)$ and a probability function $u(x,y)$ that $x$ mutation to $y$ given a
mutation has occured. The house-of-cards mutational model is one of the most interesting both biologically and
mathematically. This model states that $u(x,y)=u_{HC}(y)$. 

In a discrete time model the density of allelic types after selection is 
\begin{equation}\label{eq:sel}
p_s(x,t) = p(x,t)W(x)/\overline{W}(t).
\end{equation}
We can then calculate the density of allelic types in the next generation as
\begin{equation}\label{eq:mut}
p(x,t+1) = \left[1-\mu(x)\right]p_s(x,t) + \int_{\chi}p_s(u,t)\mu(y)u(y,x)dy.
\end{equation}
A problem we might face when choosing $W(x)$, $\mu$, and $u$ is that the distribution $p(x,t)$ might not approach an
equilibrium distribution as time proceeds. If selection is not strong enough to counteract mutation the resulting mean
or variance may go to infinity. Burger is trying to give the conditions such that this doesn't happen. Conditions W1 and
W2 do not make complete sense to me, but I think they have something to do with fitness in the discrete generation model
in particular. Conditions A-D give examples for different ways of categorizing the model. Equation 2.9 for the
HC-mutation model means that the mutation to disfavorable types relative to the mutation rate must not be too small.

B{\"u}ger in particular wants to show the proof for necessary conditions of the HC mutational model because it can
hopefully be done quite easily. He first shows that the equilibrium density can be written as 
\begin{equation} \label{eq:HCeq}
\hat{p}(x) = \frac{\mu u_{HC}(x)}{1-\frac{1-\mu}{\hat{\overline{W}}}W(x)},
\end{equation}
which is equation 2.12 in the text. This can be derived from the text equations 2.7 and 2.8 (\ref{eq:sel}
and \ref{eq:mut} here).
\begin{align}
\hat{p}(x) &= (1-\mu)p_S(x) + \int_{\chi}p_S(x) \mu u_{HC}(x)dy \nonumber \\
           &= (1-\mu)p_S(x) + \mu u_{HC}(x) \nonumber\\
           &= (1-\mu)\hat{p}(x)W(x)/\hat{\overline{W}} + \mu u_{HC}(x).
\end{align}
This rearranges easily to equation \ref{eq:HCeq}. To understand the conditions on $\hat{\overline{W}}$ we need to think a
little bit more about the fitness function $W$. It is clear from equation \ref{eq:sel} that multiplying the fitness
function by a constant will not change the dynamics. If we let $x_0$ be the optimum type in the population, then we can
scale $W$ such that $W(x_0)$ is one. Doing this means that $0<W(x)\leq 1$ because we don't want to give any types zero
fitness. Returning to eqaution \ref{eq:HCeq} this implies that $1-\mu \leq \hat{\overline{W}} < 1$ because we want
$\hat{p}$ to be positive. 

The big goal here is to prove that $\hat{p}$ has a density when 
\begin{equation}
\mu \int_{\chi}\frac{u_{HC}(x)}{1-W(y)}dy \geq 1.
\end{equation}
What we know to begin with is that equation \ref{eq:HCeq} will be a density when $1-\mu \leq \hat{\overline{W}} < 1$ and 
$\int \hat{p}(x)dx = 1$. Working off of this, we would like it to be true that there is an equilibrium density if some $b$ 
exists such that 
\begin{equation}
p_b(x) = \frac{\mu u_{HC}(x)}{1-bW(x)}
\end{equation}
integrates to one. It is easy to show that $p_b$ integrates to one if and only if $b=(1-\mu)/\overline(W)_b$. This means
that if we can find such a $b$ between $1-\mu$ and $1$ such that $\int p_b dx=1$, then that $b$ will necessarily be
$(1-\mu)/\overline(W)_b$, meaning that the density will be given by equation \ref{eq:HCeq}. If no such $b$ exists then
$\hat{p}$ cannot be a density. What remains is to find the condition such that the required $b$ exists. This is done by 
defining 
\begin{equation}
I(\beta) = \mu \int_{\chi}\frac{u_{HC}(x)}{1-\beta W(x)}.
\end{equation}
This function is strictly monotonically increasing. Therefore, if $[I(1-\mu),I(1)]$ contains one, there exists a $b$ such 
that $I(b)=1$ and $\hat{p}$ is a density. We can see that $I(1-\mu)<1$ by seeing that 
\begin{align}
\frac{1}{\mu}(1-(1-\mu)W(x))) &> 1 \nonumber \\
-(1-\mu)W(x) &> \mu - 1 \nonumber \\
(1-\mu)W(x) &< 1-\mu, 
\end{align}
because $W(x) \leq 1$. After this, all we need to know is that 
\begin{equation}
\lim_{\beta \to 1} \mu \int_{\chi}\frac{u_{HC}(x)}{1-\beta W(x)} \geq 1.
\end{equation}
This proves the condition. 

In continuous times, fitness values, $m(x)$ are Malthusian parameters instead of viabilities or fecundities, and it is 
the differences of fitness values instead of their ratio that matters as can be seen from selection equations.
\begin{equation}
p_i'=p_iW_i/\overline{W}
\end{equation}
\begin{equation}
\dot{p}_i = p_i(m_i-\overline{m})
\end{equation}
Fitness is then normalized by setting the optimum allelic value to zero. If $s$ is the length of a generation, we can
relate the two fitnesses by $W(x) = e^{s m(x)}$. Second order expansions of this are $W(x)=1+sm(x)+s^2h(x)$ and
$\overline{W}(t) = 1+s\overline{m}(t) + s^2\overline{h}(t)$. We can get a contiuous time version of the dynamical
equations by plugging these terms into equations \ref{eq:mut} and \ref{eq:sel} and taking the limit as the generation
time goes to zero. 
\begin{equation}
\lim_{s\to0}\frac{1}{s}[p(x,t+1) - p(x,t)]
\end{equation}
\begin{equation}
p(x,t+1) - p(x,t) = [1-\mu(x)]p(x,t)W(x)/\overline{W}(t) + \int p(y,t) W(x)/\overline{W}(t)\mu(y)u(y,x)dy - p(x,t)\overline{W}(t)/\overline{W}(t)
\end{equation}
Some rearrangement and plugging in the second order expansion, taking the limit as $s$ goes to zero, and rescaling the
mutation rate by the generation time gives Kimura's equation.
