Recall that the moment generating function for the distribution of trait values
from a single locus is
\begin{equation*}
  \varphi_{\mathbf{Y}}(\mathbf{k}) = \int \exp \left( \sum_{\omega \in \Omega} s_{\omega}t_{\omega} \right)
  \Pro(\mathbf{T}=\mathbf{t})\mathrm{d}\mathbf{t}
  \Bigr|_{s_{\omega}=\frac{\theta}{2} \left( \psi\left(\sum_{a \in \omega}k_{a}\right) -1 \right)}.
\end{equation*}
If we substitute in the Taylor series expansions for the moment generating
function of the trait value distribution we get
\begin{equation*}
  \int \prod_{\Omega} \exp\left[ t_{\omega} \T \left( \sum_{n=1}^{\infty} \frac{m_n}{n!}
    \left( \sum_{a \in \omega} k_a \right)^n \right) \right]
  \Pro(\mathbf{T} = \mathbf{t}) \mathrm{d}\mathbf{t}.
\end{equation*}
If we then write the Taylor series of each exponential function we get
\begin{equation*}
  \int \prod_{\Omega} \left[ \sum_{j=0}^\infty \frac{t_{\omega}^j}{j!}
  \left( \T \right)^j \left( \sum_{n=1}^{\infty} \frac{m_n}{n!}
  \left( \sum_{a \in \omega} k_a \right)^n \right)^j \right]
  \Pro(\mathbf{T} = \mathbf{t}) \mathrm{d}\mathbf{t},
\end{equation*}
which is equivalent to
\begin{equation*}
  1 + \sum_{\Omega} \E[T_\omega] \T \sum_{n=1}^{\infty} \frac{m_n}{n!} \left(
  \sum_{a \in \omega} k_a \right)^n +
  \sum_{\Omega \times \Omega} \frac{1}{2} E[T_{\omega_1}T_{\omega_2}]
  \sum_{n=1}^{\infty} \T \frac{m_n}{n!} \left(\sum_{a \in \omega_1} k_a \right)^n
  \sum_{n=1}^{\infty} \T \frac{m_n}{n!} \left(\sum_{a \in \omega_2} k_a \right)^n + \ldots
\end{equation*}

This is raised to the power $L$ for a trait controlled by $L$ loci. We want the
limit as the number of loci increases while the size of mutational decreases.
This can be expressed by the limits $L m_1 \to \mu$, $L m_2\to \sigma^2$ and $L
m_i\to 0$ for $i>2$ as $L \to \infty$. Knowing we will not be retaining $m_3$
and above we can rewrite the mgf as
\begin{equation*}
  \left(1 + \sum_{\Omega} \E[T_\omega] \T \left(m_1 \sum_{a \in \omega} k_a +
  \frac{m_2}{2} \left(\sum_{a \in \omega} k_a \right)^2\right)\right)^L.
\end{equation*}
The result of taking these limits is
\begin{equation}
  \label{eq:norm_mgf}
  \exp \left( \sum_{\omega \in \Omega}\E[T_{\omega}] \T \left( \mu 
  \sum_{a \in \omega} k_a + \frac{\sigma^2}{2}\left( \sum_{a \in \omega}
  k_a\right)^2\right)\right).
\end{equation}

This is multivariate normal distribution with mean equal to $\E[T_{MRCA}] \T \mu$,
variance equal to $\E[T_{MRCA}] \T \sigma^2$, and covariance between $Y_a$ and $Y_b$
equal to $\E[\tau_{a+b}] \T \sigma^2$. This can be seen from equation
\eqref{eq:norm_mgf} by noting that in the mgf for a multivariate normal
distribution the coefficient in the exponential of $k_a$ is the mean of $Y_a$
and the coefficient of $k_ak_b$ is $2\Cov[Y_a,Y_b]$ if $a\neq b$ and $\Var[Y_a]$
if $a=b$.

These $Y$ values are not directly observed because in the theory presented here
they are measured as the sum of differences since the $T_{MRCA}$s at the causal
loci affecting the trait. Rather, differences between individual trait values
are what analyses would be based on. Since the $Y$ are normally distributed
differences between trait values such as $Y_a-Y_b$ will be as well.
\begin{align*}
  \Var[Y_a-Y_b] &= Var[Y_a] + \Var[Y_b] - 2\Cov[Y_a,Y_b]\\
                &= 2(\E[T_{MRCA}] - \E[\tau_{a+b}])\sigma^2.
\end{align*}
This also tells us that the distribution of trait differences will be the same
in the infinitesimal limit with and without a low mutation rate approximation.

A much simpler heuristic derivation of the limiting normal distribution can be
done by calculating the variance and covariance at a single locus. This
derivation is very similar to that done by \citet{Schraiber2015}. Using the law
of total variance we can write
\begin{equation*}
  \Var[Y] = \E\left[\Var[Y|T]\right] +
  \Var\left[\E[Y|T]\right]
\end{equation*}
The variance conditional on $T$ can be calculated again using the law of total
variance and conditioning on the number of mutation at the locus. 
\begin{align*}
  \Var[Y|T] &= \E[\Var[Y|M]|T] + \Var[E[Y|M]|T]\\
            &= \E[M(m_2-m_1^2)|T] + \Var[Mm_1|T]\\
            &= \T T (m_2-m_1^2) + \T T m_1^2\\
            &= \T T m_2
\end{align*}
\begin{align*}
  \E[Y|T] = \T T m_1\\
  \Var[\T T m_1] = \left( \T m_1\right)^2\Var[T]
\end{align*}
Therefore we have
\begin{equation}
  \Var[T] = \T m_2 \E[T_{MRCA}] + (\T m_1)^2 \Var[T_{MRCA}].
\end{equation}

The same procedure can be done for the covariance.
\begin{equation*}
  \Cov[Y_a,Y_b] = \E[\Cov[Y_a,Y_b|T]] + \Cov[E[Y_a|T],E[Y_b|T]]
\end{equation*}
We can break $Y_a$ and $Y_b$ into a shared part, $Y_S$ and unshared parts for
each, $Y_{\delta a}$ and $Y_{\delta b}$.
\begin{align*}
  \Cov[Y_S + Y_{\delta a}, Y_S + Y_{\delta b}|T] = \Var[Y_s|T] = \T \tau_{a+b} m_2.\\
  E[\T \tau_{a+b} m_2] = \T m_2\E[\tau_{a+b}]
\end{align*}
\begin{align*}
  \Cov[E[Y_a|T],E[Y_b|T]] = \Cov[\T T m_1, \T T m_1] = \left( \T m_1 \right)^2\Var[T_{MRCA}]
\end{align*}
Therefore we have
\begin{equation}
  \Cov[Y_a,Y_b] = \T m_2 \E[\tau_{a+b}] + \left( \T m_1 \right)^2 \Var[T_{MRCA}].
\end{equation}

The terms proportional to the variance of the $T_{MRCA}$ disappear because
$Lm_1^2 \to 0$ as $L \to \infty$.

%%% Local Variables:
%%% TeX-master: "short_report.tex"
%%% End:
